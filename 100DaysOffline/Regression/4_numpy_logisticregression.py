# -*- coding: utf-8 -*-
"""4. numpy logisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17JotMAy02r_7iFr9z4uVrjsVgOg6Tr2W
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas
import io

# from: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html
#
# wheras linear regression provides a linear response / output logistic regression uses the sigmoid fn to project this as a classification
# => eg: what's my projected test score vs will I pass? (of *course* I'll pass!)
#
# A note of loss fns: we could use MSE here but actually we'll use Log-loss aka Cross-Entropy 
#                     (these push the classification either way using a log fn to get more pronounced results - better than simple MSE)

dataset = pandas.read_csv("data/data_classification.csv").as_matrix()
print(dataset.shape)


def sigmoid(z):
    return 1 / (1 + np.exp(-1 * z))


def predict(features, weights):
    z = np.dot(features, weights)
    return sigmoid(z)  # tadaa


def cost_function(features, labels, weights):
    '''
    Using Mean Absolute Error

    Features:(100,3)
    Labels: (100,1)
    Weights:(3,1)
    Returns 1D matrix of predictions
    Cost = ( log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)
    '''
    observations = len(labels)

    predictions = predict(features, weights)

    # Take the error when label=1
    class1_cost = -labels * np.log(predictions)

    # Take the error when label=0
    class2_cost = (1 - labels) * np.log(1 - predictions)

    # Take the sum of both costs
    cost = class1_cost - class2_cost

    # Take the average cost
    cost = cost.sum() / observations

    return cost


def update_weights(features, labels, weights, lr):
    '''
    Vectorized Gradient Descent

    Features:(200, 3)
    Labels: (200, 1)
    Weights:(3, 1)
    '''
    N = len(features)

    # 1 - Get Predictions
    predictions = predict(features, weights)

    # 2 Transpose features from (200, 3) to (3, 200)
    # So we can multiply w the (200,1)  cost matrix.
    # Returns a (3,1) matrix holding 3 partial derivatives --
    # one for each feature -- representing the aggregate
    # slope of the cost function across all observations
    gradient = np.dot(features.T, predictions - labels)

    # 3 Take the average cost derivative for each feature
    gradient /= N

    # 4 - Multiply the gradient by our learning rate
    gradient *= lr

    # 5 - Subtract from our weights to minimize cost
    weights -= gradient

    return weights


def decision_boundary(prob):
    return 1 if prob >= .5 else 0


def classify(predictions):
    '''
    input  - N element array of predictions between 0 and 1
    output - N element array of 0s (False) and 1s (True)
    '''
    return np.vectorize(decision_boundary)(predictions).flatten()


def accuracy(predicted_labels, actual_labels):
    diff = predicted_labels - actual_labels
    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))


def train(features, labels, weights, alpha, iters):
    for i in range(iters):
        weights = update_weights(features, labels, weights, alpha)

        # Calculate cost for auditing purposes
        cost = cost_function(features, labels, weights)

        # Log Progress
        if i % 1000 == 0:
            print("iter: " + str(i) + " cost: " + str(cost))
    return weights


# work it out
totalFeatures = 100
devSet = 80
testSet = 20

features = dataset[:, [0, 1]]
labels = dataset[:, 2]

# normalise the data
bias = np.ones(shape=(totalFeatures, 1))
XData = features / np.linalg.norm(features, ord=np.inf, axis=0, keepdims=True)
yData = labels / np.linalg.norm(labels, ord=np.inf, axis=0, keepdims=True)
XData = np.append(XData, bias, axis=1)

# Studied / Slept / Pass|noPass
XDev = XData[0:devSet, [0, 1, 2]]
yDev = yData[0:devSet]
XDev = XDev.reshape(devSet, 3)
yDev = yDev.reshape(devSet, 1)

XTest = XData[devSet:totalFeatures:, [0, 1, 2]]
yTest = yData[devSet:totalFeatures]
XTest = XTest.reshape(testSet, 3)
yTest = yTest.reshape(testSet, 1)

weights = np.ones((3, 1))
c = 0

alpha = 0.005
iters = 10000

print(XDev.shape)
print(yDev.shape)
weights = train(XDev, yDev, weights, alpha, iters)


devSetPoints = np.append(XDev[:, [0]], XDev[:, [1]], axis=1)
devSetPoints = np.append(devSetPoints, yDev, axis=1)

devPasses = devSetPoints[devSetPoints[:, 2] == 1.0]
devFails = devSetPoints[devSetPoints[:, 2] == 0.0]

predictions = predict(XTest, weights)

acc = accuracy(predictions, yTest)
print("Accuracy: " + str(acc))

testSetPoints = np.append(XTest[:, [0]], XTest[:, [1]], axis=1)
testSetPointsAct = np.append(testSetPoints, yTest, axis=1)
testSetPointsPrd = np.append(testSetPoints, predictions, axis=1)

testPassesAct = testSetPointsAct[testSetPointsAct[:, 2] == 1.0]
testFailsAct = testSetPointsAct[testSetPointsAct[:, 2] == 0.0]

testPassesPrd = testSetPointsPrd[testSetPointsPrd[:, 2] >= 0.5]
testFailsPrd = testSetPointsPrd[testSetPointsPrd[:, 2] < 0.5]


# purdey graph time

plt.hold(True)

fx = devFails[:, 0]  # shape: n,1
fy = devFails[:, 1]

# training / dev set
_ = plt.plot(devPasses[:, 0], devPasses[:, 1], 'o', color='black')
_ = plt.plot(devFails[:, 0], devFails[:, 1], 'x', color='black')

# test set
_ = plt.plot(testPassesPrd[:, 0], testPassesPrd[:, 1], 'o', color='red')
_ = plt.plot(testFailsPrd[:, 0], testFailsPrd[:, 1], 'x', color='red')

# _ = plt.plot(testPassesAct[:, 0], testPassesAct[:, 1], 'o', color='yellow')
# _ = plt.plot(testFailsAct[:, 0], testFailsAct[:, 1], 'x', color='yellow')



plt.show()