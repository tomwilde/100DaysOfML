# -*- coding: utf-8 -*-
"""3. numpy linearMultiVariateRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Swdq-Gz316xbKZNWXuhWVY0yHXwltwym
"""

import numpy as np
from mpl_toolkits.mplot3d.axes3d import Axes3D
import matplotlib.pyplot as plt
import pandas
from matplotlib import cm


# from: https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multivariable-regression
#
# We need a cost fn and its derivative...

dataset = pandas.read_csv("Advertising.csv").as_matrix()


def cost_function(features, targets, weights):
    N = len(targets)

    predictions = predict(features, weights)

    # Matrix math lets use do this without looping
    sq_error = (predictions - targets) ** 2

    # Return average squared error among predictions
    return 1.0 / (2 * N) * sq_error.sum()


def predict(features, weights):
    return np.dot(features, weights)


def update_weights_vectorized(X, targets, weights, lr):
    companies = len(X)

    # 1 - Get Predictions
    predictions = predict(X, weights)

    # 2 - Calculate error/loss
    error = targets - predictions

    # 3 Transpose features from (200, 3) to (3, 200)
    # So we can multiply w the (200,1)  error matrix.
    # Returns a (3,1) matrix holding 3 partial derivatives --
    # one for each feature -- representing the aggregate
    # slope of the cost function across all observations
    gradient = np.dot(-X.T, error)

    # 4 Take the average error derivative for each feature
    gradient /= companies

    # 5 - Multiply the gradient by our learning rate
    gradient *= lr

    # 6 - Subtract from our weights to minimize cost
    weights -= gradient

    return weights


def train(X, y, weights, alpha, iters):
    cost_history = []

    for i in range(iters):
        weights = update_weights_vectorized(X, y, weights, alpha)

        # Calculate cost for auditing purposes
        cost = cost_function(X, y, weights)

        # Log Progress
        if i % 100 == 0:
            print
            "iter: " + str(i) + " cost: " + str(cost)
    return weights


# work it out
y = dataset[:, 4].reshape(200, 1)
X = dataset[:, [1, 2]].reshape(200, 2)

weights = np.ones((3, 1))
c = 0

alpha = 0.005
iters = 1000

# normalise the data
bias = np.ones(shape=(200, 1))
y = y / np.linalg.norm(y, ord=np.inf, axis=0, keepdims=True)
X = X / np.linalg.norm(X, ord=np.inf, axis=0, keepdims=True)
X = np.append(X, bias, axis=1)

weights = train(X, y, weights, alpha, iters)
print(weights)

# a picture is worth a thousand words...


# set up a figure twice as wide as it is tall
fig = plt.figure(figsize=plt.figaspect(0.5))

# set up the axes for the first plot
ax = fig.add_subplot(1, 2, 1, projection='3d')

x1 = X.T[0]
y1 = X.T[1]
z1 = y

# surf = ax.plot_surface(x1, y1, z1, rstride=5, cstride=5, linewidth=0, antialiased=False)

ax.scatter3D(x1, y1, z1);

# we wantz to combine plots
plt.hold(True)

# now plot our prediction
# eqn of the form: pz = a*py + b*px + c

predictions = []

for a in range(0, 10):
    for b in range(0, 10):
        features = np.array([a/10, b/10, 0]).reshape(1, 3)
        zp = predict(features, weights)
        predictions.append([a/10, b/10 , zp[0][0]])

preds = np.array(predictions)

min = preds[:,2].min()
max = preds[:,2].max()

preds[:,2] = preds[:,2] / max

x2 = preds[:, 0].reshape(100, 1)
y2 = preds[:, 1].reshape(100, 1)
z2 = preds[:, 2].reshape(100, 1)

print(x1.shape)
print(x2.shape)

print(y1.shape)
print(y2.shape)

print(z1.shape)
print(z2.shape)

ax.scatter3D(x2, y2, z2)
plt.show()
